"""
Test de conexi√≥n para el LLM (LM Studio).
Este script comprueba que el LLM est√° disponible y funcionando correctamente.
"""
import sys
import os
import time
from datetime import datetime
import json

# Agregar el directorio src al path para poder importar los m√≥dulos
src_path = os.path.join(os.path.dirname(__file__), 'src')
sys.path.insert(0, src_path)

try:
    # Importaci√≥n directa sin referencias relativas
    from llm_client_new import LLMClient
    from config import config as config_obj
    print("‚úì Importaciones exitosas")
except ImportError as e:
    print(f"‚úó Error al importar m√≥dulos: {e}")
    print(f"Ruta src: {src_path}")
    print(f"Archivos en src: {os.listdir(src_path) if os.path.exists(src_path) else 'Directorio no encontrado'}")
    sys.exit(1)

def test_configuracion_llm():
    """Verifica que la configuraci√≥n del LLM est√© correcta."""
    print("\n" + "="*60)
    print("VERIFICANDO CONFIGURACI√ìN DEL LLM")
    print("="*60)
    print(f"URL de la API: {config_obj.LLM_API_URL}")
    print(f"Modelo: {config_obj.LLM_MODEL_NAME}")
    print(f"Max tokens: {config_obj.LLM_MAX_TOKENS}")
    print(f"Temperature: {config_obj.LLM_TEMPERATURE}")

    if config_obj.LLM_API_URL and config_obj.LLM_MODEL_NAME:
        print("‚úì Configuraci√≥n b√°sica presente")
        return True
    else:
        print("‚úó Configuraci√≥n incompleta")
        return False

def test_conexion_basica():
    """Prueba la conexi√≥n b√°sica al LLM."""
    print("\n" + "="*60)
    print("PROBANDO CONEXI√ìN B√ÅSICA AL LLM")
    print("="*60)
    
    try:
        # Crear cliente LLM
        client = LLMClient()
        print("‚úì Cliente LLM creado exitosamente")
        
        # Probar conexi√≥n
        print("Probando conexi√≥n al servidor LM Studio...")
        conexion_exitosa = client.test_conexion()
        
        if conexion_exitosa:
            print("‚úì Conexi√≥n exitosa al LM Studio")
            return True, client
        else:
            print("‚úó No se pudo conectar al LM Studio")
            print("  Aseg√∫rate de que LM Studio est√© ejecut√°ndose en http://localhost:1234")
            return False, None
            
    except Exception as e:
        print(f"‚úó Error al crear cliente o probar conexi√≥n: {e}")
        return False, None

def test_funcionalidad_llm(client):
    """Prueba la funcionalidad b√°sica del LLM."""
    print("\n" + "="*60)
    print("PROBANDO FUNCIONALIDAD DEL LLM")
    print("="*60)
    
    # Test 1: Consulta simple
    print("\n1. Probando consulta simple...")
    try:
        respuesta = client._hacer_peticion("Hola, ¬øpuedes ayudarme con un an√°lisis meteorol√≥gico?")
        if respuesta:
            print("‚úì Consulta simple exitosa")
            print(f"  Respuesta (primeros 100 caracteres): {respuesta[:100]}...")
        else:
            print("‚úó No se recibi√≥ respuesta a la consulta simple")
            return False
    except Exception as e:
        print(f"‚úó Error en consulta simple: {e}")
        return False
    
    # Test 2: An√°lisis meteorol√≥gico b√°sico
    print("\n2. Probando an√°lisis meteorol√≥gico...")
    datos_test = {
        'temperature': 25.5,
        'humidity': 78,
        'dew_point': 18.2,
        'pressure': 1013.2,
        'wind_speed': 5.5
    }
    probabilidad_lluvia = 0.65
    
    try:
        analisis = client.generar_analisis_prediccion(datos_test, probabilidad_lluvia)
        if analisis:
            print("‚úì An√°lisis meteorol√≥gico generado exitosamente")
            print(f"  An√°lisis (primeros 150 caracteres): {analisis[:150]}...")
        else:
            print("‚úó No se pudo generar an√°lisis meteorol√≥gico")
            return False
    except Exception as e:
        print(f"‚úó Error en an√°lisis meteorol√≥gico: {e}")
        return False
    
    # Test 3: Explicaci√≥n de gr√°ficas
    print("\n3. Probando explicaci√≥n de gr√°ficas...")
    try:
        explicacion = client.explicar_graficas('radar', datos_test, probabilidad_lluvia)
        if explicacion:
            print("‚úì Explicaci√≥n de gr√°ficas generada exitosamente")
            print(f"  Explicaci√≥n (primeros 100 caracteres): {explicacion[:100]}...")
        else:
            print("‚úó No se pudo generar explicaci√≥n de gr√°ficas")
            return False
    except Exception as e:
        print(f"‚úó Error en explicaci√≥n de gr√°ficas: {e}")
        return False
    
    # Test 4: Reporte meteorol√≥gico
    print("\n4. Probando reporte meteorol√≥gico...")
    try:
        # Crear objeto de ubicaci√≥n requerido por generar_reporte_completo
        ubicacion = {
            'lat': 24.8019,
            'lon': -107.3940  # Coordenadas de Culiac√°n
        }
        reporte = client.generar_reporte_completo(datos_test, probabilidad_lluvia, ubicacion)
        if reporte:
            print("‚úì Reporte meteorol√≥gico generado exitosamente")
            print(f"  Reporte (primeros 150 caracteres): {reporte[:150]}...")
        else:
            print("‚úó No se pudo generar reporte meteorol√≥gico")
            return False
    except Exception as e:
        print(f"‚úó Error en reporte meteorol√≥gico: {e}")
        return False
    
    return True

def test_performance_llm(client):
    """Prueba el rendimiento del LLM."""
    print("\n" + "="*60)
    print("PROBANDO RENDIMIENTO DEL LLM")
    print("="*60)
    
    # Medir tiempo de respuesta
    consultas_test = [
        "¬øQu√© factores indican alta probabilidad de lluvia?",
        "Explica brevemente c√≥mo interpretar la humedad relativa.",
        "¬øCu√°l es la diferencia entre temperatura y punto de roc√≠o?"
    ]
    
    tiempos = []
    
    for i, consulta in enumerate(consultas_test, 1):
        print(f"\nConsulta {i}: {consulta[:50]}...")
        
        inicio = time.time()
        try:
            respuesta = client._hacer_peticion(consulta)
            fin = time.time()
            
            if respuesta:
                tiempo_respuesta = fin - inicio
                tiempos.append(tiempo_respuesta)
                print(f"‚úì Respuesta en {tiempo_respuesta:.2f} segundos")
            else:
                print("‚úó Sin respuesta")
                
        except Exception as e:
            print(f"‚úó Error: {e}")
    
    if tiempos:
        tiempo_promedio = sum(tiempos) / len(tiempos)
        print(f"\nüìä Estad√≠sticas de rendimiento:")
        print(f"  Tiempo promedio de respuesta: {tiempo_promedio:.2f} segundos")
        print(f"  Tiempo m√≠nimo: {min(tiempos):.2f} segundos")
        print(f"  Tiempo m√°ximo: {max(tiempos):.2f} segundos")
        
        if tiempo_promedio < 10:
            print("‚úì Rendimiento bueno (< 10s)")
        elif tiempo_promedio < 30:
            print("‚ö† Rendimiento aceptable (10-30s)")
        else:
            print("‚ö† Rendimiento lento (> 30s)")
    
    return len(tiempos) == len(consultas_test)

def guardar_log_test(resultados):
    """Guarda los resultados del test en un archivo log."""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = f"test_llm_log_{timestamp}.txt"
    
    try:
        with open(log_file, 'w', encoding='utf-8') as f:
            f.write(f"Test de conexi√≥n LLM - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("="*60 + "\n\n")
            
            for test, resultado in resultados.items():
                f.write(f"{test}: {'‚úì EXITOSO' if resultado else '‚úó FALLIDO'}\n")
            
            f.write(f"\nConfiguracion utilizada:\n")
            f.write(f"  URL API: {config_obj.LLM_API_URL}\n")
            f.write(f"  Modelo: {config_obj.LLM_MODEL_NAME}\n")
            f.write(f"  Max tokens: {config_obj.LLM_MAX_TOKENS}\n")
            f.write(f"  Temperature: {config_obj.LLM_TEMPERATURE}\n")
        
        print(f"\nüìÑ Log guardado en: {log_file}")
        
    except Exception as e:
        print(f"‚ö† No se pudo guardar el log: {e}")

def main():
    """Funci√≥n principal que ejecuta todos los tests."""
    print("üß™ INICIANDO TEST DE CONEXI√ìN LLM")
    print("="*60)
    print(f"Fecha y hora: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    resultados = {}
    
    # Test 1: Configuraci√≥n
    resultados['Configuraci√≥n'] = test_configuracion_llm()
    
    # Test 2: Conexi√≥n b√°sica
    conexion_exitosa, client = test_conexion_basica()
    resultados['Conexi√≥n b√°sica'] = conexion_exitosa
    
    if conexion_exitosa and client:
        # Test 3: Funcionalidad
        resultados['Funcionalidad'] = test_funcionalidad_llm(client)
        
        # Test 4: Rendimiento
        resultados['Rendimiento'] = test_performance_llm(client)
    else:
        print("\n‚ö† Saltando tests de funcionalidad y rendimiento debido a fallo de conexi√≥n")
        resultados['Funcionalidad'] = False
        resultados['Rendimiento'] = False
    
    # Resumen final
    print("\n" + "="*60)
    print("RESUMEN FINAL DEL TEST")
    print("="*60)
    
    tests_exitosos = sum(1 for resultado in resultados.values() if resultado)
    total_tests = len(resultados)
    
    for test, resultado in resultados.items():
        status = "‚úì EXITOSO" if resultado else "‚úó FALLIDO"
        print(f"{test}: {status}")
    
    print(f"\nResultado general: {tests_exitosos}/{total_tests} tests exitosos")
    
    if tests_exitosos == total_tests:
        print("üéâ ¬°Todos los tests pasaron! El LLM est√° funcionando correctamente.")
    elif tests_exitosos > 0:
        print("‚ö† Algunos tests fallaron. Revisar configuraci√≥n o estado del LM Studio.")
    else:
        print("‚ùå Todos los tests fallaron. El LLM no est√° disponible.")
    
    # Guardar log
    guardar_log_test(resultados)
    
    # Mostrar sugerencias si hay fallos
    if tests_exitosos < total_tests:
        print("\nüí° SUGERENCIAS:")
        if not resultados['Conexi√≥n b√°sica']:
            print("  1. Verificar que LM Studio est√© ejecut√°ndose")
            print("  2. Confirmar que el servidor est√© en http://localhost:1234")
            print("  3. Verificar que un modelo est√© cargado en LM Studio")
        
        if not resultados['Funcionalidad']:
            print("  4. Verificar que el modelo est√© respondiendo correctamente")
            print("  5. Revisar logs de LM Studio por errores")
        
        if not resultados['Rendimiento']:
            print("  6. Considerar usar un modelo m√°s peque√±o para mejor rendimiento")
            print("  7. Verificar recursos del sistema (RAM, CPU)")

if __name__ == "__main__":
    main()
